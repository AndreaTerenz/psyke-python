{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52126f3",
   "metadata": {},
   "source": [
    "#Â PSyKE's demo for regression comparison\n",
    "\n",
    "Some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b710e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import exists\n",
    "from psyke import Extractor\n",
    "import time\n",
    "from psyke.utils import Target\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading black-box models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PATH = \"../test/resources/\"\n",
    "datasets = [\"contingency\", \"contingency\", \"anticipate\", \"anticipate\", \"contingency\", \"anticipate\"]\n",
    "models = [\n",
    "    \"CONTINGENCY_no_input-memory_DecisionTree_MaxDepth10\",\n",
    "    \"CONTINGENCY_no_input-time_DecisionTree_MaxDepth10\",\n",
    "    \"ANTICIPATE_no_input-memory_DecisionTree_MaxDepth10\",\n",
    "    \"ANTICIPATE_no_input-time_DecisionTree_MaxDepth10\",\n",
    "    \"CONTINGENCY_input-cost_DecisionTree_MaxDepth15\",\n",
    "    \"ANTICIPATE_input-cost_DecisionTree_MaxDepth15\"\n",
    "]\n",
    "models = [\n",
    "    pickle.load(open(f\"{PATH}predictors/{dataset}/{path}\", 'rb')) for path, dataset in zip(models, datasets)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to pre-process data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def process(dataset):\n",
    "    df = pd.read_csv(f\"{PATH}datasets/{dataset}_trainDataset.csv\")\n",
    "\n",
    "    # Removes header entries\n",
    "    df = df[df['sol(keuro)'] != 'sol(keuro)']\n",
    "\n",
    "    # Fixed stuff which is always there\n",
    "    df['PV(kW)'] = df['PV(kW)'].map(lambda entry: entry[1:-1].split())\n",
    "    df['PV(kW)'] = df['PV(kW)'].map(lambda entry: list(np.float_(entry)))\n",
    "    df['Load(kW)'] = df['Load(kW)'].map(lambda entry: entry[1:-1].split())\n",
    "    df['Load(kW)'] = df['Load(kW)'].map(lambda entry: list(np.float_(entry)))\n",
    "\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    X['PV_mean'] = df['PV(kW)'].map(lambda entry: np.array(entry).mean())\n",
    "    X['PV_std'] = df['PV(kW)'].map(lambda entry: np.array(entry).std())\n",
    "    X['Load_mean'] = df['Load(kW)'].map(lambda entry: np.array(entry).mean())\n",
    "    X['Load_std'] = df['Load(kW)'].map(lambda entry: np.array(entry).std())\n",
    "    X['nScenarios'] = df['nScenarios']\n",
    "    X['cost'] = df['sol(keuro)']\n",
    "    X['time'] = df['time(sec)']\n",
    "    X['memo'] = df['memAvg(MB)']\n",
    "\n",
    "    X.to_csv(f\"{PATH}datasets/{dataset}.csv\", index = False)\n",
    "\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experiment setting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "toRemove = [\n",
    "    ['PV_mean', 'PV_std', 'Load_mean', 'Load_std', 'time', 'cost'],\n",
    "    ['PV_mean', 'PV_std', 'Load_mean', 'Load_std', 'memo', 'cost'],\n",
    "    ['PV_mean', 'PV_std', 'Load_mean', 'Load_std', 'time', 'cost'],\n",
    "    ['PV_mean', 'PV_std', 'Load_mean', 'Load_std', 'memo', 'cost'],\n",
    "    [\"time\", \"memo\"],\n",
    "    [\"time\", \"memo\"]\n",
    "]\n",
    "\n",
    "features = [\n",
    "    [\"nTraces\"],\n",
    "    [\"nTraces\"],\n",
    "    [\"nScenarios\"],\n",
    "    [\"nScenarios\"],\n",
    "    ['PV_mean', 'PV_std', 'Load_mean', 'Load_std', 'nTraces'],\n",
    "    ['PV_mean', 'PV_std', 'Load_mean', 'Load_std', 'nScenarios']\n",
    "]\n",
    "\n",
    "targets = [\"memo\", \"time\", \"memo\", \"time\", \"cost\", \"cost\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Computational time assessment for ORCHiD w.r.t. different amounts of input features and input instances.\n",
    "Data averaged on 100 executions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contingency memo 1 ['PV_mean', 'PV_std', 'Load_mean', 'Load_std'] ['time', 'cost']\n",
      "\n",
      "5 variables\n",
      "\n",
      "100 instances: 0.06 +- 0.03 sec\n",
      "500 instances: 0.06 +- 0.01 sec\n",
      "1000 instances: 0.09 +- 0.03 sec\n",
      "2000 instances: 0.17 +- 0.03 sec\n",
      "4000 instances: 0.36 +- 0.02 sec\n",
      "7000 instances: 0.92 +- 0.08 sec\n",
      "10000 instances: 1.68 +- 0.15 sec\n",
      "\n",
      "4 variables\n",
      "\n",
      "100 instances: 0.07 +- 0.01 sec\n",
      "500 instances: 0.07 +- 0.01 sec\n",
      "1000 instances: 0.09 +- 0.01 sec\n",
      "2000 instances: 0.15 +- 0.02 sec\n",
      "4000 instances: 0.37 +- 0.03 sec\n",
      "7000 instances: 0.97 +- 0.07 sec\n",
      "10000 instances: 1.76 +- 0.07 sec\n",
      "\n",
      "3 variables\n",
      "\n",
      "100 instances: 0.06 +- 0.01 sec\n",
      "500 instances: 0.08 +- 0.01 sec\n",
      "1000 instances: 0.09 +- 0.01 sec\n",
      "2000 instances: 0.14 +- 0.02 sec\n",
      "4000 instances: 0.33 +- 0.03 sec\n",
      "7000 instances: 0.83 +- 0.07 sec\n",
      "10000 instances: 1.75 +- 0.12 sec\n",
      "\n",
      "2 variables\n",
      "\n",
      "100 instances: 0.04 +- 0.01 sec\n",
      "500 instances: 0.05 +- 0.01 sec\n",
      "1000 instances: 0.08 +- 0.01 sec\n",
      "2000 instances: 0.13 +- 0.01 sec\n",
      "4000 instances: 0.32 +- 0.03 sec\n",
      "7000 instances: 0.84 +- 0.11 sec\n",
      "10000 instances: 1.69 +- 0.09 sec\n",
      "\n",
      "1 variables\n",
      "\n",
      "100 instances: 0.07 +- 0.01 sec\n",
      "500 instances: 0.06 +- 0.01 sec\n",
      "1000 instances: 0.09 +- 0.01 sec\n",
      "2000 instances: 0.12 +- 0.01 sec\n",
      "4000 instances: 0.34 +- 0.03 sec\n",
      "7000 instances: 0.56 +- 0.04 sec\n",
      "10000 instances: 1.34 +- 0.17 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "rem, feat, target, dataset, model = toRemove[i], features[i], targets[i], datasets[i], models[i]\n",
    "\n",
    "print(dataset, target, len(feat), rem[:-2], rem[-2:])\n",
    "print()\n",
    "name = f\"{PATH}datasets/{dataset}.csv\"\n",
    "\n",
    "if not exists(name):\n",
    "    process(dataset)\n",
    "\n",
    "dataset = pd.read_csv(name).drop(rem[-2 :], axis = 1)\n",
    "\n",
    "for r in range(-1, len(rem) - 2):\n",
    "    if r >= 0:\n",
    "        dataset = dataset.drop([rem[r]], axis = 1)\n",
    "\n",
    "    train, test = train_test_split(dataset, test_size=0.1, random_state=10)\n",
    "    model.fit(train.iloc[:, :-1], train.iloc[:, -1])\n",
    "\n",
    "    print(f\"{len(dataset.columns) - 1} variables\\n\")\n",
    "    for j in [100, 500, 1000, 2000, 4000, 7000, 10000]:\n",
    "        res = []\n",
    "        for i in range(100):\n",
    "            print(f\"{j} instances: {i} runs\", end=\"\\r\")\n",
    "            t0 = time.time()\n",
    "            orchid = Extractor.orchid(model, depth=1, error_threshold=.8, output=Target.REGRESSION)\n",
    "            _ = orchid.extract(train.iloc[:j, :])\n",
    "            t1 = time.time()\n",
    "            res.append(t1 - t0)\n",
    "        res = np.array(res)\n",
    "        print(f'{j} instances: {np.mean(res):.2f} +- {np.std(res):.2f} sec')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}